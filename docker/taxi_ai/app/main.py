import logging
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("TaxiAi")

app = FastAPI()

# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸ NER
MODEL_PATH = "/app/model/bert-base-multilingual-cased-ner-hrl"
try:
    logger.info("Loading tokenizer from %s", MODEL_PATH)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    logger.info("Loading model from %s", MODEL_PATH)
    model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)
    logger.info("Creating NER pipeline")
    ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")
    logger.info("âœ… Multilingual NER model loaded successfully from %s", MODEL_PATH)
except Exception as e:
    logger.error("âŒ Failed to load NER model: %s", str(e))
    ner_pipeline = None

class TextRequest(BaseModel):
    text: str

class SpacyEntity(BaseModel):
    text: str
    label: str

class AiInnerResponse(BaseModel):
    entities_hf: list[dict[str, str]] = []
    entities_spacy: list[SpacyEntity] = []
    origin: str | None = None
    destination: str | None = None
    details: list[str] = []

class AiResponse(BaseModel):
    text: str
    response: AiInnerResponse

@app.post("/parse", response_model=AiResponse)
async def process_text(request: TextRequest):
    logger.info("ðŸ“© Received request with text: %s", request.text)
    response = AiResponse(
        text=request.text,
        response=AiInnerResponse(entities_hf=[], entities_spacy=[], origin=None, destination=None, details=[])
    )

    if ner_pipeline is None:
        logger.error("NER pipeline is not initialized")
        return response

    try:
        logger.info("Running NER pipeline on text: %s", request.text)
        entities = ner_pipeline(request.text)
        logger.info("NER pipeline returned: %s", entities)
        response.response.entities_hf = [
            {"text": ent["word"], "label": ent["entity_group"]} for ent in entities
        ]

        # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ ORIGIN Ð¸ DESTINATION Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
        origins = []
        destinations = []
        for i, ent in enumerate(entities):
            if ent["entity_group"] == "LOC":
                # Ð Ð°ÑÑˆÐ¸Ñ€ÑÐµÐ¼ Ð¾ÐºÐ½Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð´Ð¾ 5 ÑÐ»Ð¾Ð²
                preceding_text = request.text[:ent["start"]].lower().split()[-5:]
                logger.info("Processing entity: %s, preceding text: %s", ent["word"], preceding_text)
                if any(prep in preceding_text for prep in ["Ñ", "Ð¸Ð·", "from", "Ð·", "Ð²Ñ–Ð´", "Ð·Ñ–", "with"]):
                    origins.append(ent["word"])
                elif any(prep in preceding_text for prep in ["Ð²", "Ð´Ð¾", "to", "Ð½Ð°", "Ñƒ", "into", "towards"]):
                    destinations.append(ent["word"])
                else:
                    # Ð•ÑÐ»Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð³ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½, ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ Ð²Ñ‚Ð¾Ñ€ÑƒÑŽ Ð»Ð¾ÐºÐ°Ñ†Ð¸ÑŽ destination
                    if len(origins) > 0 and not destinations:
                        destinations.append(ent["word"])

        response.response.origin = origins[0] if origins else None
        response.response.destination = destinations[0] if destinations else None

        # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð¾Ð¿Ñ†Ð¸Ð¹
        details = []
        text_lower = request.text.lower()
        if any(word in text_lower for word in ["Ð·Ð°Ð²Ñ‚Ñ€Ð°", "tomorrow"]):
            details.append("tomorrow")
        if any(word in text_lower for word in ["Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¹ Ñ‡ÐµÐ¼Ð¾Ð´Ð°Ð½", "large luggage", "Ð²ÐµÐ»Ð¸ÐºÐ° Ð²Ð°Ð»Ñ–Ð·Ð°"]):
            details.append("large luggage")
        response.response.details = details

        logger.info("Extracted: origin=%s, destination=%s, details=%s",
                   response.response.origin, response.response.destination, details)
    except Exception as e:
        logger.error("Error in NER processing: %s", str(e))

    return response
